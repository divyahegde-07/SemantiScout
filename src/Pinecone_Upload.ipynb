{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0DYMfL48hXN",
        "outputId": "94bde5a9-c687-482f-c345-7c1f0c0039ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/419.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pinecone --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1HTSey5F7EE",
        "outputId": "3251e910-3d18-4bc6-8a9e-aaa9e2f2e603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRGrLflZ5RKD"
      },
      "outputs": [],
      "source": [
        "#Import Necessary Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pinecone\n",
        "import multiprocessing as mp\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModelWithProjection, CLIPTextModelWithProjection\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK3HuT4DO4BO",
        "outputId": "59f6a8c2-9c04-417a-f316-96bb9e268147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            " 25%|██▌       | 1/4 [00:30<01:30, 30.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Dataframe /content/drive/MyDrive/Office Products Embeddings/Office_Products_tokenized_results_1.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [01:00<01:00, 30.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Dataframe /content/drive/MyDrive/Office Products Embeddings/Office_Products_tokenized_results_2.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [01:30<00:30, 30.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Dataframe /content/drive/MyDrive/Office Products Embeddings/Office_Products_tokenized_results_3.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:59<00:00, 29.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Dataframe /content/drive/MyDrive/Office Products Embeddings/Office_Products_tokenized_results_4.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "#Identify Blank and Non-Blank Images\n",
        "img = Image.new('RGB', (200, 200), color='white')\n",
        "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  image_inputs = processor(images=img, return_tensors=\"pt\")\n",
        "  # img_tensor = model.get_image_features(**image_inputs)\n",
        "  img_tensor = model(**image_inputs).image_embeds.squeeze(0)\n",
        "  white_img_array = img_tensor.detach().numpy()\n",
        "\n",
        "def white_non_white(x, white_img_array):\n",
        "  if (np.round(x, decimals = 2) == np.round(white_img_array, decimals = 2)).all():\n",
        "    return 'Blank Image'\n",
        "  else:\n",
        "    return 'Valid Image'\n",
        "\n",
        "\n",
        "#Create a List of Dataframes\n",
        "df_list = []\n",
        "\n",
        "for i in tqdm(range(4)):\n",
        "  df_name = '/content/drive/MyDrive/Office Products Embeddings/Office_Products_tokenized_results_' + str(i+1) + '.parquet'\n",
        "  df = pd.read_parquet(df_name)\n",
        "  df['text_embeddings'] = df['tokenized_text']\n",
        "  df['image_embeddings'] = df['tokenized_image']\n",
        "\n",
        "  df = df.loc[df['similarity'] > 0.2]\n",
        "  df = df.reset_index(drop = True)\n",
        "\n",
        "  for j in range(len(df)):\n",
        "    # df.loc[j, 'image_type'] = white_non_white(df.loc[i, 'image_embeddings'], white_img_array, df.loc[i, 'image_url'])\n",
        "    df.loc[j, 'image_type'] = white_non_white(df.loc[i, 'image_embeddings'], white_img_array)\n",
        "\n",
        "  df_list.append(df)\n",
        "  print(f\"Completed Dataframe {df_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep only products that do not have white images\n",
        "for df in tqdm(df_list):\n",
        "  df = df.loc[df['image_type'] == 'Valid Image']\n",
        "  df = df.reset_index(drop = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhKvRNv42_9w",
        "outputId": "44015446-0a9b-4cec-8a66-af8145c51eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 13.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get count of total number of Observations remaining\n",
        "length = 0\n",
        "\n",
        "for i in df_list:\n",
        "  length += len(i)\n",
        "\n",
        "length"
      ],
      "metadata": {
        "id": "yzCl38EKV7XS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Pslu0V-QqWc"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "pinecone_api = \"API KEY GOES HERE\"\n",
        "\n",
        "pc = pinecone.Pinecone(api_key=pinecone_api)\n",
        "index = pc.Index(\"PINECONE INDEX NAME GOES HERE\",\n",
        "                 host = \"HOST URL OF PINECONE INDEX GOES HERE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if any product exceeds max upload size limit of Pinecone per vector\n",
        "import json\n",
        "\n",
        "for i, row in tqdm(df_list[0].iterrows()):\n",
        "  batch = []\n",
        "  total_size = 0\n",
        "  vector_id = row['parent_asin']\n",
        "\n",
        "  # Extract vector (assuming it's a numpy array)\n",
        "  # vector = (row['text_embeddings'] + row['image_embeddings'])\n",
        "  # vector = np.concatenate((row['text_embeddings'], row['image_embeddings']), axis = 0)\n",
        "  vector = row['image_embeddings']\n",
        "\n",
        "\n",
        "  metadata = {\n",
        "              'parent_asin': row['parent_asin'],\n",
        "              'title': row['title'],\n",
        "              'description': row['description'],\n",
        "              # 'features': row['features'],\n",
        "              'image_url': row['image_url'],\n",
        "              'text_image_cosine_similarity': row['similarity'],\n",
        "              # 'image_type': row['image_type']\n",
        "          }\n",
        "\n",
        "\n",
        "  metadata_json = json.dumps(metadata)\n",
        "  total_size = len(metadata_json.encode('utf-8'))\n",
        "\n",
        "  if total_size > 40960:\n",
        "    print(i, row, total_size)\n",
        "\n",
        "  counter = 0\n",
        "\n",
        "  #40960 bytes is the max metadata size that a vector can have on Pinecone\n",
        "  #If size exceeds limit, reduce description length\n",
        "  while total_size > 40960:\n",
        "    new_length = int(len(metadata['description'])/2)\n",
        "    metadata['description'] = row['description'][:new_length]\n",
        "    metadata_json = json.dumps(metadata)\n",
        "    total_size = len(metadata_json.encode('utf-8'))\n",
        "    print(total_size, len(metadata['description']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdutI3t9ZfXT",
        "outputId": "4df3fc2d-e7a6-44c5-b89e-7495b488be92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8943it [00:00, 11239.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6575 parent_asin                                                B00006BBEL\n",
            "title               HP Premium Plus Photo Paper, High Gloss (50, 8...\n",
            "description         Product Description, HP's best all-around phot...\n",
            "image_url           https://m.media-amazon.com/images/I/5153BV-O6T...\n",
            "tokenized_text      [0.16396813, -0.41250056, 0.12949368, -0.31431...\n",
            "tokenized_image     [0.77177644, 0.042132437, 0.33375576, -0.35702...\n",
            "similarity                                                   0.337588\n",
            "text_embeddings     [0.16396813, -0.41250056, 0.12949368, -0.31431...\n",
            "image_embeddings    [0.77177644, 0.042132437, 0.33375576, -0.35702...\n",
            "image_type                                                Valid Image\n",
            "Name: 6575, dtype: object 82171\n",
            "41223 40948\n",
            "20743 20474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "55234it [00:04, 12104.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52886 parent_asin                                                B001664XLY\n",
            "title                     Lexmark X2600 All -in-One Printer (12L1685)\n",
            "description         Overview \\t\\t\\t \\t\\t\\t \\t\\t \\t \\t\\t \\t\\t\\t \\t\\...\n",
            "image_url           https://m.media-amazon.com/images/I/41Q4z-DrGr...\n",
            "tokenized_text      [0.03120926, -0.2801697, -0.03177153, -0.28221...\n",
            "tokenized_image     [0.19849995, -0.18025938, 0.39231557, 0.081128...\n",
            "similarity                                                   0.327669\n",
            "text_embeddings     [0.03120926, -0.2801697, -0.03177153, -0.28221...\n",
            "image_embeddings    [0.19849995, -0.18025938, 0.39231557, 0.081128...\n",
            "image_type                                                Valid Image\n",
            "Name: 52886, dtype: object 84235\n",
            "42322 41849\n",
            "21393 20924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "99361it [00:08, 11391.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEESh9jPR0zH",
        "outputId": "d7261a4a-a5d6-44f7-ea9e-eb32ab89527f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "89673it [05:10, 288.39it/s]\n",
            "99361it [05:36, 295.23it/s]\n",
            "99336it [05:43, 289.52it/s]\n",
            "99364it [05:43, 289.13it/s]\n"
          ]
        }
      ],
      "source": [
        "#Using Multiple CPU Cores to upsertion to Pinecone\n",
        "import sys\n",
        "import json\n",
        "\n",
        "\n",
        "# Function to handle upsertion for each chunk\n",
        "def upload_chunk_to_pinecone(dataframe_chunk):\n",
        "    # text_batch = []\n",
        "    # image_batch = []\n",
        "    batch = []\n",
        "\n",
        "    for i, row in tqdm(dataframe_chunk.iterrows()):\n",
        "        total_size = 0\n",
        "\n",
        "        # Define unique vector ID (e.g., 'vector_0', 'vector_1', etc.)\n",
        "\n",
        "        vector_id = row['parent_asin']\n",
        "\n",
        "        # Extract vector (assuming it's a numpy array)\n",
        "        # vector = (row['text_embeddings'] + row['image_embeddings'])/2 #Average\n",
        "        # vector = row['text_embeddings'] + row['image_embeddings'] #Addition\n",
        "        # vector = np.concatenate((row['text_embeddings'], row['image_embeddings']), axis = 0) #Concatenation\n",
        "        vector = row['image_embeddings'] #Only Image\n",
        "\n",
        "\n",
        "        # Create metadata dictionary\n",
        "        metadata = {\n",
        "            'parent_asin': row['parent_asin'],\n",
        "            'title': row['title'],\n",
        "            'description': row['description'],\n",
        "            # 'features': row['features'],\n",
        "            'image_url': row['image_url'],\n",
        "            'text_image_cosine_similarity': row['similarity'],\n",
        "            # 'image_type': row['image_type']\n",
        "        }\n",
        "\n",
        "\n",
        "        max_size = 40960\n",
        "\n",
        "        metadata_json = json.dumps(metadata)\n",
        "        total_size += len(metadata_json.encode('utf-8'))\n",
        "\n",
        "        #40960 bytes is the max metadata size that a vector can have on Pinecone\n",
        "        #If size exceeds limit, reduce description length\n",
        "        while total_size > 40960:\n",
        "          new_length = int(len(metadata['description'])/2)\n",
        "          metadata['description'] = row['description'][:new_length]\n",
        "          metadata_json = json.dumps(metadata)\n",
        "          total_size = len(metadata_json.encode('utf-8'))\n",
        "        batch.append((vector_id, vector, metadata))\n",
        "\n",
        "        # Upload batch in chunks of 500 vectors ---> Switch to 200 when using concat\n",
        "        if len(batch) == 500:\n",
        "          index.upsert(vectors=batch)\n",
        "          batch = []\n",
        "\n",
        "    # Upload any remaining vectors in the batch\n",
        "    if batch:\n",
        "        index.upsert(vectors=batch)\n",
        "\n",
        "\n",
        "# Function to perform multiprocessing across all dataframes\n",
        "def parallel_upload(dataframes, num_processes=2):\n",
        "    with mp.Pool(num_processes) as pool:\n",
        "        pool.map(upload_chunk_to_pinecone, dataframes)\n",
        "\n",
        "\n",
        "parallel_upload(df_list, num_processes=4)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}